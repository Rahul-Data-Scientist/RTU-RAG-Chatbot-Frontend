# RAG Frontend

**Streamlit frontend** for the RTU syllabus **RAG Chatbot**, allowing users to select **semester, subject, and unit**, ask questions, and receive **streaming answers** from the backend.

---

## ğŸ—‚ï¸ Repo Structure

```
root/
â”œâ”€â”€ .github/workflows/deploy.yml   # CI/CD workflow
â”œâ”€â”€ .streamlit/config.toml         # Streamlit config
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile                     # Docker image build
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ semester_subjects.json         # Mapping: semester â†’ subjects
â”œâ”€â”€ subjects_mapping.json          # Mapping: subject name â†’ subject code
â”œâ”€â”€ subjects_units.json            # Mapping: subject code â†’ units
â””â”€â”€ streamlit_ui.py                # Streamlit UI and chat logic
```

---

## âš¡ Features

* Interactive **Streamlit UI** for RAG queries
* Dropdowns for **semester, subject, and unit selection**
* Maintains **chat history** during session
* Streams **LLM responses** from the backend in real-time
* CI/CD deployment via **GitHub Actions â†’ AWS ECR â†’ EC2**

---

## ğŸ–¥ï¸ Frontend Architecture

```mermaid
flowchart LR
    A[User in Browser]:::nodeStyle --> B[Streamlit UI]:::nodeStyle
    B --> C[Select Semester, Subject, Unit]:::nodeStyle
    B --> D[Send Query to Backend]:::nodeStyle
    D --> E[RAG Backend]:::nodeStyle
    E --> F[Query Rewrite & Filtered Retrieval from Qdrant]:::nodeStyle
    F --> G[Relevant Chunks & Context Construction]:::nodeStyle
    G --> H[Streaming LLM Response]:::nodeStyle
    H --> I[Back to Streamlit UI]:::nodeStyle
    I --> A[Display Answer]:::nodeStyle

    classDef nodeStyle fill:#bbf,stroke:#333,stroke-width:2px,color:#000
```

---

### **Flow Explanation**

1. **User selects** semester, subject, and unit in Streamlit sidebar.
2. **User asks a question** â†’ Streamlit sends it to the backend via `/rag/query`.
3. **Backend processes query**:

   * Query rewrite for better retrieval
   * Filtered retrieval from **Qdrant**
   * Context construction
   * Streaming LLM response generation
4. **Streaming response** is sent back to Streamlit UI.
5. User sees **real-time answer**, with chat history maintained.

---

## ğŸ“¦ Dependencies

* `streamlit` â†’ UI
* `requests` â†’ Communicate with backend
* JSON files â†’ Populate dropdowns for semester, subject, and unit

---

## âš™ï¸ CI/CD Workflow Overview

* Triggered on push to `master` branch
* Steps:

  1. Checkout code
  2. Configure AWS credentials
  3. Login to ECR
  4. Build Docker image
  5. Push image to ECR
  6. Deploy container on EC2

---

## âœ… Notes

* Streams backend responses **in real-time**
* Maintains **chat history** for context-aware answers
* Safe to deploy via CI/CD as workflow automatically pulls the latest image

---
