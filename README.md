# RAG Frontend

**Streamlit frontend** for the RTU syllabus **RAG Chatbot**, allowing users to select **semester, subject, and unit**, ask questions, and receive **streaming answers** from the backend.

---

## üóÇÔ∏è Repo Structure

```
root/
‚îú‚îÄ‚îÄ .github/workflows/deploy.yml   # CI/CD workflow
‚îú‚îÄ‚îÄ .streamlit/config.toml         # Streamlit config
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Dockerfile                     # Docker image build
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ semester_subjects.json         # Mapping: semester ‚Üí subjects
‚îú‚îÄ‚îÄ subjects_mapping.json          # Mapping: subject name ‚Üí subject code
‚îú‚îÄ‚îÄ subjects_units.json            # Mapping: subject code ‚Üí units
‚îî‚îÄ‚îÄ streamlit_ui.py                # Streamlit UI and chat logic
```

---

## ‚ö° Features

* Interactive **Streamlit UI** for RAG queries
* Dropdowns for **semester, subject, and unit selection**
* Maintains **chat history** during session
* Streams **LLM responses** from the backend in real-time
* CI/CD deployment via **GitHub Actions ‚Üí AWS ECR ‚Üí EC2**

---

## üñ•Ô∏è Frontend Architecture

```mermaid
flowchart LR
    A[User in Browser] --> B[Streamlit UI (streamlit_ui.py)]
    B --> C[Select Semester, Subject, Unit]
    B --> D[Send Query to Backend (/rag/query)]
    D --> E[RAG Backend (FastAPI)]
    E --> F[Query Rewrite ‚Üí Filtered Retrieval from Qdrant]
    F --> G[Relevant Chunks ‚Üí Context Construction]
    G --> H[Streaming LLM Response (GPT-4.1-nano)]
    H --> I[Stream Response Back to Streamlit UI]
    I --> A[Display Streaming Answer to User]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style D fill:#bfb,stroke:#333,stroke-width:2px
    style H fill:#ffb,stroke:#333,stroke-width:2px
```

---

### **Flow Explanation**

1. **User selects** semester, subject, and unit in Streamlit sidebar.
2. **User asks a question** ‚Üí Streamlit sends it to the backend via `/rag/query`.
3. **Backend processes query**:

   * Query rewrite for better retrieval
   * Filtered retrieval from **Qdrant**
   * Context construction
   * Streaming LLM response generation
4. **Streaming response** is sent back to Streamlit UI.
5. User sees **real-time answer**, with chat history maintained.

---


## üì¶ Dependencies

* `streamlit` ‚Üí UI
* `requests` ‚Üí Communicate with backend
* JSON files ‚Üí Populate dropdowns for semester, subject, and unit

---

## ‚öôÔ∏è CI/CD Workflow Overview

* Triggered on push to `master` branch
* Steps:

  1. Checkout code
  2. Configure AWS credentials
  3. Login to ECR
  4. Build Docker image
  5. Push image to ECR
  6. Deploy container on EC2

---

## ‚úÖ Notes

* Streams backend responses **in real-time**
* Maintains **chat history** for context-aware answers
* Safe to deploy via CI/CD as workflow automatically pulls the latest image

---
